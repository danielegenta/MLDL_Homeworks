{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GentaDaniele_Homework2-MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a4cd9ea3db04e3dbae02d5fa604099b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6986b6b1fdc547f9a827926430b5d72a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a35efd51f3724b709cb44ad474a9ea17",
              "IPY_MODEL_b424d4b6a25946469cd72736050f2a9f"
            ]
          }
        },
        "6986b6b1fdc547f9a827926430b5d72a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a35efd51f3724b709cb44ad474a9ea17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74aaf57a35ca40c0a40ca7684ad29843",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244418560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244418560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f17751c2db64494e9f7ed21a3e477ed7"
          }
        },
        "b424d4b6a25946469cd72736050f2a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05cf44aa16e040f8b7b4138f9057c49c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:03&lt;00:00, 76.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e7edbc0b1e3465391137c7aee6fc3fa"
          }
        },
        "74aaf57a35ca40c0a40ca7684ad29843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f17751c2db64494e9f7ed21a3e477ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05cf44aa16e040f8b7b4138f9057c49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e7edbc0b1e3465391137c7aee6fc3fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "outputId": "b6a75abe-59da-410b-d95f-c26cc331fa9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "outputId": "bbf4a5e1-0c7e-4588-e8ae-8759ec397318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import copy\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-2       # The initial Learning Rate, ORIGINAL: 1e-3\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset), original: 30\n",
        "STEP_SIZE = 40      # How many epochs before decreasing learning rate (if using a step-down policy), original: 20\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "# MY PARAMS\n",
        "LAYERS = 'FC' #NORMAL, CONVOLUTIONAL, FC\n",
        "DATA_AUGMENTATION = False #TRUE, FALSE\n",
        "CHOSEN_NETWORK = 'ALEXNET' #ALEXNET, VGG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6cAt76Azzl",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing (Pretrained AlexNet)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dftkAsUgAyQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_pretrained = (0.485, 0.456, 0.406)\n",
        "stDev_pretrained = (0.229, 0.224, 0.225)\n",
        "img_eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean = mean_pretrained,std= stDev_pretrained)\n",
        "])\n",
        "\n",
        "\n",
        "img_train_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean = mean_pretrained,std= stDev_pretrained)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "037-UtLctraR",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY_G5Vhftq9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA AUGMENTATION TRANSFORMATIONS\n",
        "\n",
        "# 1 - RANDOM CROP\n",
        "random_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.RandomCrop(224),  # Crops a random square patch of the image => difference from before\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean_pretrained, stDev_pretrained) # Normalizes tensor with mean and standard deviation\n",
        "                                      ])\n",
        "\n",
        "# 2 - HORIZONTAL FLIP\n",
        "flip_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.RandomHorizontalFlip(0.5), \n",
        "                                      transforms.CenterCrop(224),  # Crops a random square patch of the image                                      \n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean_pretrained, stDev_pretrained) # Normalizes tensor with mean and standard deviation\n",
        "                                      ])\n",
        "# 3 - RANDOM FLIP -- not used \n",
        "\"\"\"random_flip_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.RandomCrop(224),  # Crops a random square patch of the image\n",
        "                                      transforms.RandomHorizontalFlip(1.0), \n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean_pretrained, stDev_pretrained) # Normalizes tensor with mean and standard deviation\n",
        "                                      ])\"\"\"\n",
        "\n",
        "# 4 - GRAY SCALE\n",
        "greyscale_transform = transforms.Compose([transforms.RandomGrayscale(0.5),\n",
        "                                      transforms.Resize(256), \n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(), \n",
        "                                      transforms.Normalize(mean=mean_pretrained, std=stDev_pretrained) \n",
        "                                      ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8yDlC8RIVy4",
        "colab_type": "text"
      },
      "source": [
        "**Caltech.py embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhbrDsONpl8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        "\n",
        "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None, split='train', transf='None'):\n",
        "    images = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
        "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "    if extensions is not None:\n",
        "        def is_valid_file(x):\n",
        "            return x.lower().endswith(extensions)\n",
        "\n",
        "    inFile = os.path.join(dir, split + '.txt')\n",
        "    with open(inFile, 'r') as f:      \n",
        "      inImages = f.read().splitlines()\n",
        "\n",
        "    root = os.path.join(dir, '101_ObjectCategories/')\n",
        "\n",
        "    for fname in sorted(inImages):\n",
        "      fpath = os.path.split(fname)\n",
        "      target = fpath[0]\n",
        "      path = os.path.join(root, fname)\n",
        "      if is_valid_file(path) and target != 'BACKGROUND_Google':\n",
        "        image = pil_loader(path)        \n",
        "        if transf is not None:\n",
        "            image = transf(image) # Applies preprocessing\n",
        "        item = (image, class_to_idx[target])\n",
        "        images.append(item)\n",
        "\n",
        "    return images\n",
        "\n",
        "class Caltech(VisionDataset):\n",
        "    def __init__(self, root, split='train', transform=None, target_transform=None):\n",
        "        super(Caltech, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "        self.split = split # This defines the split you are going to use\n",
        "                           # (split files are called 'train.txt' and 'test.txt')\n",
        "\n",
        "        classes, class_to_idx = self._find_classes(self.root)\n",
        "        samples = make_dataset(self.root, class_to_idx, IMG_EXTENSIONS, split=self.split, transf=transform)\n",
        "        if len(samples) == 0:\n",
        "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
        "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
        "\n",
        "\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.samples = samples\n",
        "        self.root = root\n",
        "        self.targets = [s[1] for s in samples]\n",
        "\n",
        "    def _find_classes(self, dir):        \n",
        "        root = os.path.join(dir, '101_ObjectCategories/')\n",
        "        if sys.version_info >= (3, 5):\n",
        "            # Faster and available in Python 3.5 and above\n",
        "            classes = [d.name for d in os.scandir(root) if d.is_dir()]\n",
        "        else:          \n",
        "            classes = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "        \n",
        "        classes.remove('BACKGROUND_Google')\n",
        "        classes.sort()                \n",
        "        #print(classes)\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        __getitem__ should access an element through its index\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sample, target) where target is class_index of the target class.\n",
        "        '''\n",
        "\n",
        "        image, label = self.samples[index]        \n",
        "        return image, label\n",
        "\n",
        "    def getAllTargets(self):\n",
        "      return self.targets\n",
        "\n",
        "    def __len__(self):        \n",
        "        return len(self.samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KENxA5FrcUIx",
        "colab_type": "code",
        "outputId": "8d2aa6a6-e025-40df-b49b-dac7658707ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from math import floor\n",
        "import numpy as np\n",
        "\n",
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "  !mv 'Homework2-Caltech101' 'Caltech101'\n",
        "\n",
        "DATA_DIR = 'Caltech101/'\n",
        "TRAIN_SPLIT = 'train'\n",
        "TEST_SPLIT = 'test'\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split=TRAIN_SPLIT,  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split=TEST_SPLIT, transform=eval_transform)\n",
        "\n",
        "#from Caltech101.caltech_dataset import Caltech"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Homework2-Caltech101'...\n",
            "remote: Enumerating objects: 9256, done.\u001b[K\n",
            "remote: Total 9256 (delta 0), reused 0 (delta 0), pack-reused 9256\u001b[K\n",
            "Receiving objects: 100% (9256/9256), 129.48 MiB | 40.23 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Checking out files: 100% (9149/9149), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpMvw5Skxt92",
        "colab_type": "text"
      },
      "source": [
        "**Train and validation split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWAYLTu6xptz",
        "colab_type": "code",
        "outputId": "7c020aac-b478-4977-ce65-bbd3dde1e1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_dataset_len = len(train_dataset)\n",
        "val_dataset_size = 0.5\n",
        "\n",
        "# 2A\n",
        "y = np.array(train_dataset.getAllTargets())\n",
        "X = np.zeros(len(y))\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=val_dataset_size, random_state=42)\n",
        "sss.get_n_splits(X, y)\n",
        "\n",
        "# just one iteration\n",
        "for train_indexes, val_indexes in sss.split(X, y):\n",
        "  val_dataset = Subset(train_dataset, val_indexes)\n",
        "  splitted_train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "print('Train Dataset: {}'.format(len(splitted_train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJJ1GUaEIc32",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataset (pretrained AlexNet)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAWCHsiHIbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare Pytorch train/test Datasets\n",
        "img_train_dataset = Caltech(DATA_DIR, split=TRAIN_SPLIT,  transform=img_train_transform)\n",
        "img_test_dataset = Caltech(DATA_DIR, split=TEST_SPLIT, transform=img_eval_transform)\n",
        "\n",
        "# data augmentation (done only for pretrained net)\n",
        "if (DATA_AUGMENTATION):\n",
        "\n",
        "  # RANDOM CROP\n",
        "  augmented_train_dataset = Caltech(DATA_DIR, split='train', transform=random_transform)\n",
        "  augmented_test_dataset = Caltech(DATA_DIR, split='test', transform=random_transform)\n",
        "  # no need to data aug the test set\n",
        "  img_train_dataset.samples += augmented_train_dataset.samples\n",
        "\n",
        "  # HORIZONTAL FLIP\n",
        "  augmented_train_dataset = Caltech(DATA_DIR, split='train', transform=flip_transform)\n",
        "  augmented_test_dataset = Caltech(DATA_DIR, split='test', transform=flip_transform)\n",
        "  img_train_dataset.samples += augmented_train_dataset.samples\n",
        "\n",
        "  # RANDOM FLIP\n",
        "  \"\"\"augmented_train_dataset = Caltech(DATA_DIR, split='train', transform=random_flip_transform)\n",
        "  augmented_test_dataset = Caltech(DATA_DIR, split='test', transform=random_flip_transform)\n",
        "  img_train_dataset.samples += augmented_train_dataset.samples\"\"\"\n",
        "\n",
        "  # GREYSCALE TRANSFORMATION\n",
        "  \"\"\"augmented_train_dataset = Caltech(DATA_DIR, split='train', transform=greyscale_transform)\n",
        "  augmented_test_dataset = Caltech(DATA_DIR, split='test', transform=greyscale_transform)\n",
        "  img_train_dataset.samples += augmented_train_dataset.samples\"\"\"\n",
        "\n",
        "\n",
        "img_splitted_train_dataset = Subset(train_dataset, train_indexes)\n",
        "img_val_dataset = Subset(train_dataset, val_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(splitted_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCKKYADhI5_r",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders (pretrained AlexNet)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weNdhbkdI32f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_train_dataloader = DataLoader(img_splitted_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "img_val_dataloader = DataLoader(img_val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "img_test_dataloader = DataLoader(img_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network (both scratch and pretrained)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN",
        "colab_type": "code",
        "outputId": "111a970d-92c6-4785-cb31-f9f58a82f427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "3a4cd9ea3db04e3dbae02d5fa604099b",
            "6986b6b1fdc547f9a827926430b5d72a",
            "a35efd51f3724b709cb44ad474a9ea17",
            "b424d4b6a25946469cd72736050f2a9f",
            "74aaf57a35ca40c0a40ca7684ad29843",
            "f17751c2db64494e9f7ed21a3e477ed7",
            "05cf44aa16e040f8b7b4138f9057c49c",
            "5e7edbc0b1e3465391137c7aee6fc3fa"
          ]
        }
      },
      "source": [
        "if CHOSEN_NETWORK == 'ALEXNET':\n",
        "  net = alexnet() # Loading AlexNet model\n",
        "  bestnet = alexnet() # Instance where the best network will be copied for the validation phase\n",
        "  pretrainednet = alexnet(pretrained=True) # Instance where the best network will be copied for the validation phase\n",
        "elif CHOSEN_NETWORK == 'VGG':\n",
        "  net = vgg16() \n",
        "  bestnet = vgg16() \n",
        "  pretrainednet = vgg16(pretrained=True)\n",
        "\n",
        "\n",
        "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
        "# We need 101 outputs for Caltech-101\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "bestnet.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "pretrainednet.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is strongly suggested to study torchvision.models.alexnet source code"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a4cd9ea3db04e3dbae02d5fa604099b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training (both scratch and pretrained)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MOST OF THE LINES OF THIS BLOCK HAVE BEEN EMBEDDED IN THE COMMON FUNCTION\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# i could squeeze all of the below in the common function\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "# In this case we optimize over all the parameters of AlexNet\n",
        "bestnet_parameters_to_optimize = bestnet.parameters()\n",
        "if (LAYERS == 'NORMAL'): # complete architecture\n",
        "  pretrainednet_parameters_to_optimize = pretrainednet.parameters()\n",
        "  parameters_to_optimize = net.parameters() \n",
        "elif (LAYERS == 'CONVOLUTIONAL'): # freeze fully connected\n",
        "  pretrainednet_parameters_to_optimize = pretrainednet.features.parameters()\n",
        "  parameters_to_optimize = net.features.parameters() \n",
        "elif (LAYERS == 'FC'): # freeze covolutional\n",
        "  pretrainednet_parameters_to_optimize = pretrainednet.classifier.parameters()\n",
        "  parameters_to_optimize = net.classifier.parameters()\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "bestnet_optimizer = optim.SGD(bestnet_parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "pretrainednet_optimizer = optim.SGD(pretrainednet_parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "bestnet_scheduler = optim.lr_scheduler.StepLR(bestnet_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "pretrainednet_scheduler = optim.lr_scheduler.StepLR(pretrainednet_optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ2tP-M-Fga0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validateTest(nnet, loader, dataset):\n",
        "  nnet = nnet.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  nnet.train(False) # Set Network to evaluation mode\n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels in tqdm(loader):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = nnet(images)\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(dataset))\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3QDdAHJIzoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output(accuracy, accuraciesValidation, accuraciesTraining, lossesValidation, epochs_list):\n",
        "  print('FINAL: Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "  ###########\n",
        "  # Plots\n",
        "  ###########\n",
        "  epochsAccuracies = list(zip(epochs_list, accuraciesValidation, accuraciesTraining))\n",
        "  dataEpochsAccuracy = pd.DataFrame(epochsAccuracies, columns = ['Epochs', 'Accuracy validation', 'Accuracy training'])\n",
        "\n",
        "  # epochs against accuracy\n",
        "  ax = sns.lineplot(x=\"Epochs\", y=\"Accuracy validation\", data=dataEpochsAccuracy)\n",
        "  ax = sns.lineplot(x=\"Epochs\", y=\"Accuracy training\", data=dataEpochsAccuracy)\n",
        "  ax.legend(['Accuracy validation','Accuracy training'])\n",
        "  ax.set(xlabel='Epochs', ylabel='Accuracy')\n",
        "  plt.show()\n",
        "  \n",
        "  # epochs against loss\n",
        "  epochsLosses = list(zip(epochs_list, lossesValidation))\n",
        "  dataEpochsLosses = pd.DataFrame(epochsLosses, columns = ['Epochs', 'Loss'])\n",
        "\n",
        "  ax1 = sns.lineplot(x=\"Epochs\", y=\"Loss\", data=dataEpochsLosses)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJzJlerjlcN",
        "colab_type": "text"
      },
      "source": [
        "***Common function***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpVExJKojkt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainValidateTest(mode, selNet, selOptimizer, selScheduler, selLR, selNEpochs, selStepSize, selTrainLoader, selValLoader, selTestLoader, selTrainingDataset, selValidationDataset, selTestDataset):\n",
        "  # By default, everything is loaded to cpu\n",
        "\n",
        "  selNet = selNet.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "\n",
        "  training_accuracies = []\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  epochs_list = []\n",
        "\n",
        "  # keep track of accuracy and loss, in order to save the best trained network\n",
        "  # i start from the worst scenario possible\n",
        "  best_accuracy = float('-inf') #minimum possible value in Python\n",
        "  best_loss = float('inf')\n",
        "\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(selNEpochs):\n",
        "    print('\\nStarting epoch {}/{}, LR = {}'.format(epoch+1, selNEpochs, selScheduler.get_lr()))\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in selTrainLoader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      selNet.train() # Sets module in training mode\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "      selOptimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "      outputs = selNet(images)\n",
        "\n",
        "      # Compute loss based on output and ground truth\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      selOptimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "      current_step += 1\n",
        "\n",
        "    # Training set accuracy\n",
        "    training_accuracy = validateTest(selNet, selTrainLoader, selTrainingDataset)\n",
        "    training_accuracies.append(training_accuracy)\n",
        "\n",
        "    ###################\n",
        "    # VALIDATION PHASE\n",
        "    ###################\n",
        "    # (at the end of each epoch)\n",
        "    # implementing of model selection with validation\n",
        "\n",
        "    accuracy = validateTest(selNet, selValLoader, selValidationDataset)\n",
        "    print('Validation Accuracy: {}'.format(accuracy))\n",
        "    accuracies.append(accuracy)\n",
        "    losses.append(loss.item())\n",
        "    epochs_list.append(epoch)\n",
        "\n",
        "    # shoud i update my best model?\n",
        "    # i choose based on accuracy and loss (in this order)\n",
        "    if accuracy > best_accuracy:\n",
        "      bestnet = copy.deepcopy(selNet)\n",
        "      best_accuracy = accuracy\n",
        "    elif accuracy == best_accuracy:\n",
        "      if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        bestnet = copy.deepcopy(selNet) \n",
        "    \n",
        "    # Step the scheduler\n",
        "    selScheduler.step() \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #############\n",
        "  # TEST PHASE\n",
        "  #############\n",
        "  # i want to test on the best model\n",
        " \n",
        "  accuracy = validateTest(bestnet, selTestLoader, selTestDataset)\n",
        "\n",
        "  ##########\n",
        "  # Output\n",
        "  ##########\n",
        "  output(accuracy, accuracies, training_accuracies, losses, epochs_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmyO8HeJmP7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train from scratch\n",
        "\n",
        "#trainValidateTest(\"Training from scratch\", net, optimizer, scheduler, LR, NUM_EPOCHS, STEP_SIZE, train_dataloader, val_dataloader, test_dataloader,splitted_train_dataset, val_dataset, test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5YnzITnltJ",
        "colab_type": "code",
        "outputId": "594ca96c-3bc2-4310-87a4-44f6cbaed5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 3A-B-C\n",
        "# transfer learning\n",
        "\n",
        "trainValidateTest(\"Transfer Learning\",pretrainednet, pretrainednet_optimizer, pretrainednet_scheduler, LR, NUM_EPOCHS, STEP_SIZE, img_train_dataloader, img_val_dataloader, img_test_dataloader,img_splitted_train_dataset, img_val_dataset, img_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 4.860447883605957\n",
            "Step 10, Loss 1.8766950368881226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  4.07it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6414246196403873\n",
            "\n",
            "Starting epoch 2/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 20, Loss 0.7005923390388489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.81it/s]\n",
            "100%|██████████| 12/12 [00:03<00:00,  3.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7793914246196404\n",
            "\n",
            "Starting epoch 3/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 30, Loss 0.4092668294906616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  4.02it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7942600276625172\n",
            "\n",
            "Starting epoch 4/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 40, Loss 0.2198803722858429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.77it/s]\n",
            "100%|██████████| 12/12 [00:03<00:00,  4.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8091286307053942\n",
            "\n",
            "Starting epoch 5/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 50, Loss 0.10416561365127563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.76it/s]\n",
            "100%|██████████| 12/12 [00:03<00:00,  3.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.822268326417704\n",
            "\n",
            "Starting epoch 6/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 60, Loss 0.08423440158367157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.74it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8226141078838174\n",
            "\n",
            "Starting epoch 7/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 70, Loss 0.09682168811559677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.95it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.822268326417704\n",
            "\n",
            "Starting epoch 8/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 80, Loss 0.0450066402554512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.822268326417704\n",
            "\n",
            "Starting epoch 9/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 90, Loss 0.053278036415576935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.98it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8215767634854771\n",
            "\n",
            "Starting epoch 10/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 100, Loss 0.04528672993183136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  4.00it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8253803596127247\n",
            "\n",
            "Starting epoch 11/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 110, Loss 0.04928011819720268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/11 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 120, Loss 0.03186044842004776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.98it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8257261410788381\n",
            "\n",
            "Starting epoch 12/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 130, Loss 0.027917973697185516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.99it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8319502074688797\n",
            "\n",
            "Starting epoch 13/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 140, Loss 0.028302334249019623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.97it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.828838174273859\n",
            "\n",
            "Starting epoch 14/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 150, Loss 0.022677630186080933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.96it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.826417704011065\n",
            "\n",
            "Starting epoch 15/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 160, Loss 0.024910669773817062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.97it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8298755186721992\n",
            "\n",
            "Starting epoch 16/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 170, Loss 0.04398346319794655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  4.01it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8284923928077456\n",
            "\n",
            "Starting epoch 17/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 180, Loss 0.021927911788225174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.96it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.830567081604426\n",
            "\n",
            "Starting epoch 18/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 190, Loss 0.011022910475730896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8309128630705395\n",
            "\n",
            "Starting epoch 19/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 200, Loss 0.01699308678507805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.99it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8291839557399724\n",
            "\n",
            "Starting epoch 20/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 210, Loss 0.03745055943727493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.99it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8340248962655602\n",
            "\n",
            "Starting epoch 21/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 220, Loss 0.016998574137687683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/11 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 230, Loss 0.011977896094322205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.92it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8284923928077456\n",
            "\n",
            "Starting epoch 22/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 240, Loss 0.01678524538874626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8284923928077456\n",
            "\n",
            "Starting epoch 23/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 250, Loss 0.010392922908067703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8274550484094052\n",
            "\n",
            "Starting epoch 24/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 260, Loss 0.02531241998076439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8267634854771784\n",
            "\n",
            "Starting epoch 25/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 270, Loss 0.009104408323764801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.98it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8295297372060858\n",
            "\n",
            "Starting epoch 26/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 280, Loss 0.009954266250133514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.94it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.826417704011065\n",
            "\n",
            "Starting epoch 27/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 290, Loss 0.006418071687221527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.93it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8278008298755186\n",
            "\n",
            "Starting epoch 28/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 300, Loss 0.007024712860584259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.99it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.828838174273859\n",
            "\n",
            "Starting epoch 29/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 310, Loss 0.016538284718990326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  4.00it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8291839557399724\n",
            "\n",
            "Starting epoch 30/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 320, Loss 0.00983181968331337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.97it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.14it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8316044260027663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:02<00:00,  4.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FINAL: Test Accuracy: 0.8302799861735223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvanjrUUdEcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3D - Experiment by training only the fully connected layers (freeze other layers)\n",
        "#trainValidateTest(\"Transfer Learning\",\"FC\", pretrainednet, LR, NUM_EPOCHS, STEP_SIZE, img_train_dataloader, img_val_dataloader, img_test_dataloader, img_val_dataset, img_test_dataset)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw7hA2Kae-0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3E - Experiment by training only the convolutional layers (freeze other layers)\n",
        "#trainValidateTest(\"Transfer Learning\",\"CONVOLUTIONAL\", pretrainednet, LR, NUM_EPOCHS, STEP_SIZE, img_train_dataloader, img_val_dataloader, img_test_dataloader, img_val_dataset, img_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}